{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# web-scraper-with-python-and-selenium\n",
    "\n",
    "In this project will cover how to start extracting data with Selenium and Python. We will build a Python script that will log in to a website, scrape some data, format it nicely, and store it in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An overview of Selenium\n",
    "Selenium is a suite of tools for automating web browsers that was first introduced as a tool for cross-browser testing.\n",
    "\n",
    "The `API` built by the Selenium team uses the `WebDriver` protocol to take control of a web browser, like Chrome or Firefox, and perform different tasks, like:\n",
    "\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking screenshots\n",
    "- Clicking buttons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be wondering how all this translates into web scraping. It’s simple, really.\n",
    "\n",
    "Data extraction can be a real pain in the neck sometimes. Websites are being built as Single Page Applications nowadays even when there’s no need for that. They’re popping `CAPTCHAs` more frequently than needed and even `blocking regular users’ IPs`.\n",
    "In short, `bot detection` is a very `frustrating` feature that feels like a bug.\n",
    "\n",
    "`Selenium` can help in these cases by understanding and executing `Javascript` code and automating many tedious processes of web scraping, like `scrolling` through the page, grabbing `HTML elements`, or `exporting fetched data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To show the real power of Selenium and Python, we are going to scrape some information off the /r/learnprogramming subreddit. Besides scraping data, I’ll also show you how signing in can be implemented. Now that we have an understanding of the primary tool and the website we are going to use, let’s see what other requisites we need to have installed:\n",
    "\n",
    "- Python\n",
    "- Selenium package\n",
    "- Pandas package\n",
    "- BeautifulSoup package\n",
    "- Google Chrome\n",
    "- Chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium --upgrade --quiet\n",
    "! pip install pandas --upgrade --quiet\n",
    "! pip install bs4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also run Google Chrome without a graphical user interface and `log the page’s HTML content` by adding a couple of lines of code. We will set the headless option to true for the chrome driver (to remove the graphical interface) and a `window size of 1080 pixels` (to get the correct HTML code for our use case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.reddit.com/r/learnprogramming/top/?t=month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.page_source)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined code will look like. The last two lines of code exit Chrome right after finishing logging the page’s HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.reddit.com/r/learnprogramming/top/?t=month\")\n",
    "\n",
    "print(driver.page_source)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebElement\n",
    "\n",
    "A `WebElement` is a Selenium object that represents an HTML element. As you will see in the following tutorial, we can perform many actions on these elements. Some of them are:\n",
    "\n",
    "- Clicking on it by using the .click() method\n",
    "- Providing text to a specific input element by calling the .send_keys() method\n",
    "- Reading the text of an element by using element.text\n",
    "- Checking if an element is displayed on the page by calling .is_displayed() on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of Selenium in action\n",
    "Now that we have our project set up, we can finally get to scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging in\n",
    "\n",
    "We are going to showcase the power of Selenium by logging in to our Reddit account and scraping the previously presented data. Let’s start by making Selenium click on the login button at the top of the page. After inspecting the page’s HTML, we can see that the login button’s Xpath name is `//*[@id=\"SHORTCUT_FOCUSABLE_DIV\"]/div[1]/header/div/div[2]/div/div[1]/a[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in\n",
    "login_button= driver.find_element(By.XPATH, '//*[@id=\"SHORTCUT_FOCUSABLE_DIV\"]/div[1]/header/div/div[2]/div/div[1]/a[1]')\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open up the login modal where we can see the user and password inputs we have to fill up. Let’s continue with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to login frame\n",
    "login_frame = driver.find_element(By.XPATH,'/html/body/div/main/div[1]')\n",
    "driver.switch_to_frame(login_frame)\n",
    "\n",
    "# enter user name\n",
    "username=driver.find_element(By.XPATH,'//*[@id=\"loginUsername\"]')\n",
    "username.send_keys('')\n",
    "time.sleep(3)\n",
    "\n",
    "#enter password\n",
    "password = driver.find_element(By.XPATH,'//*[@id=\"loginPassword\"]')\n",
    "password.send_keys('')\n",
    "time.sleep(3)\n",
    "\n",
    "#enter submit button\n",
    "submit = driver.find_element(By.XPATH,'/html/body/div/main/div[1]/div/div/form/fieldset[4]/button')\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the `modal` element, we can see that its container is an `iframe`. This is why we have to switch to frame in the first part of the code, as selecting the inputs without it will result in an error.\n",
    "\n",
    "Next, we get the input elements and provide them with the proper credentials before hitting the submit button. This will bring us back to the /r/learnprogramming page, but now we are logged in and ready to upvote!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a screenshot\n",
    "\n",
    "Taking a screenshot using Selenium and Python is pretty easy. All you have to do is write the following command in the  after declaring the web driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.save_screenshot('screenshot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s useful to know that you can set the Google Chrome window size by adding the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data\n",
    "\n",
    "As we have previously stated, we need to get the posts’ title, author, and number of upvotes. Let’s start with the help of BeautifulSoup and Pandas packages and creating three empty arrays for every type of information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "upvotes=[]\n",
    "authors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use BeautifulSoup to parse the HTML document by writing the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully inspecting the HTML document and choosing the right selectors, we are now going to fetch the titles, upvotes, and authors and assign them to the right array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in div:\n",
    "    title = i.find('h3', attrs={'class':'_eYtD2XCVieq6emjKBH3m'})\n",
    "    upvote = i.find('div', attrs={'class': '_1E9mcoVn4MYnuBQSVDt1gC'})\n",
    "    author = i.find('a', attrs={'class': '_23wugcdiaj44hdfugIAlnX'})\n",
    "    titles.append(title.text)\n",
    "    upvotes.append(upvote.text)\n",
    "    authors.append(author.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will store the information in a CSV file using the Pandas package we imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Post title': titles, 'Author': authors, 'Number of upvotes': upvotes})\n",
    "df.to_csv('posts.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus tip**: Sometimes, we need more data than the website provides on the first load. Most of the time, the fetching data action fires when the user scrolls down. If you need to scroll down to get more data, you can use the .execute_script() method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrollDown = \"window.scrollBy(0,2000);\"\n",
    "driver.execute_script(scrollDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"web-scraper-with-python-and-selenium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}